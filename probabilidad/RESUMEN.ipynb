{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. La probabilidad como área de estudio es:\n",
    "    - `NO`: Un área de las matemáticas que nos enseña que la incertidumbre solo es cuantificable cuando conocemos todas las variables necesarias de\n",
    "    - La probabilidad es un lenguaje que nos permite cuantificar la incertidumbre\n",
    "\n",
    "\n",
    "2. Las dos escuelas principales de pensamiento probabilístico son:\n",
    "    - Frecuentista y bayesiana.\n",
    "\n",
    "3. En machine learning las principales fuentes de incertidumbre de un modelo son:\n",
    "    - `NO`: Solamente los datos sobre los que se aplica un modelo.\n",
    "    - Datos, Atributos del modelo, Arquitectura del modelo\n",
    "\n",
    "4. La probabilidad conjunta es:\n",
    "    - La probabilidad de dos o más eventos aleatorios.\n",
    "\n",
    "5. La probabilidad condicional P(A | B) se interpreta como:\n",
    "    - La probabilidad de que suceda A sabiendo que ha sucedido B.\n",
    "\n",
    "6. La expresión matemática que describe correctamente la regla del producto es:\n",
    "    - $P(A ∩ B) = P(A|B)P(B)$\n",
    "\n",
    "7. Cuando se calcula una probabilidad condicional, el efecto que la condición tiene sobre el espacio muestral es:\n",
    "    - Reduce el espacio muestral.\n",
    "\n",
    "8. Considera un juego de ruleta de dos jugadores apostando sobre 8 opciones diferentes, tenemos que el jugador 1 tiene su apuesta A= {2,4,6,8} y el jugador 2 apostó por las casillas B = {1,2,3,4}, entonces la probabilidad de que gane el jugador 1 sabiendo que la bolita cayó en una de las opciones de B es:\n",
    "    - $P(1 | B) = 1/2$\n",
    "\n",
    "9. Una distribución de probabilidad es:\n",
    "    - Una función matemática que asigna a cada posible ocurrencia de una variable aleatoria un número que llamamos la probabilidad de dicha ocurrencia.\n",
    "\n",
    "10. Si consideramos 5 lanzamientos de moneda (p=0.5) consecutivos, la probabilidad de obtener 3 caras es:\n",
    "    - 5/16\n",
    "\n",
    "11. ¿Cuál es la probabilidad de obtener 2 caras o menos a partir de 3 lanzamientos de moneda (p=0.5) ?\n",
    "    - `NO`: 3/8\n",
    "\n",
    "12. Si consideramos una variable aleatoria que sigue una distribución gaussiana con media igual a 4 y desviación estándar igual a 0.3, usando la función norm() de scipy.stats, la densidad de probabilidad de que dicha variable tenga el valor 0.2 está dada por:\n",
    "    - norm(4,0.3).pdf(0.2)\n",
    "\n",
    "13. Si consideramos una variable aleatoria que sigue una distribución gaussiana con media igual a 4 y desviación estándar igual a 0.3, usando la función norm() de scipy.stats, la probabilidad acumulada de que dicha variable tenga el valor 0.2 o menor está dada por:\n",
    "    - norm(4,0.3).cdf(0.2)\n",
    "\n",
    "14. En el método de estimación paramétrica de una distribución de probabilidad:\n",
    "    - `No`: Aproximamos la distribución de los datos a partir de sumas de gaussianas.\n",
    "    - La estimacion paramétrica consiste en suponer una función para la distribución y ajustar los parámetros de los datos a dicha distribución\n",
    "\n",
    "15. El método de estimación no paramétrica se usa cuando:\n",
    "    - Los datos no siguen ninguna distribución de probabilidad conocida.\n",
    "\n",
    "16. En MLE escogemos los parámetros de la distribución de manera que:\n",
    "    - La distribución resultante, dados los datos y los parámetros, resulta en las máximas probabilidades posibles.\n",
    "\n",
    "17. En el caso de la regresión lineal, el uso de MLE es equivalente a:\n",
    "    - El método de mínimos cuadrados.\n",
    "\n",
    "18. La función de error que se usa en regresión logística se conoce como:\n",
    "    - `NO`: Error logarítmico medio\n",
    "    - Cross-Entropy: Función de error que busca siempre minimizarse en un problema de clasificación binaria. Es una consecuencia de la estimación de Máxima Verosimilitud.\n",
    "\n",
    "19. Es la representación matemática del teorema de Bayes:\n",
    "    - $P(A|B) = [P(B|A)P(A)]/P(B)$\n",
    "\n",
    "20. Dada una verosimilitud P(D|h) donde D = {x1, x2, …, xn} es un conjunto de datos y h es una hipótesis de modelamiento sobre esos datos, la hipótesis de Naive Bayes implica que:\n",
    "    - $P(D|h)=P(x1|h) * P(x2|h)* ... * (xn|h)$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
